code :

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib as mpl
import matplotlib.pylab as pylab
import numpy as np
%matplotlib inline
from nltk.tokenize import sent_tokenize, word_tokenize
import warnings
warnings.filterwarnings(action = 'ignore')
import gensim
from gensim.models import Word2Vec
import re
import bs4 as bs
import urllib.request
import nltk
nltk.download('punkt')
nltk.download('stopwords')


scrapped_data=urllib.request.urlopen("https://en.wikipedia.org/wiki/Machine_learning")
article=scrapped_data.read()
paresed_article=bs.BeautifulSoup(article,'lxml')
paragraphs=paresed_article.find_all('p')
article_text=""
for p in paragraphs:
  article_text+=p.text
sentences=article_text
print(article_text)


sentences="""Alice 23 opened the door and found that it led into a
small 90
passage, not much larger than a rat-hole: she knelt down and
looked along the passage into the loveliest garden you ever saw.
How she longed to get out of that dark hall, and wander about
among those beds of bright flowers and those cool fountains, but
she could not even get her head through the doorway; `and even if
my head would go through,' (thought) $poor Alice, `it would be of
very little use without my shoulders. Oh, how I wish
I could shut up like a telescope! I think I could, if I only
know how to begin.' For, you see, so many out-of-the-way things
had happened lately, that Alice had begun to think that very few
things indeed were really impossible.
"""
sentences = re.sub('[^A-Za-z0-9]+', ' ', sentences)
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()
print(sentences)



# remove special characters
sentences = re.sub('[^A-Za-z]+', ' ', sentences)
# remove 1 letter words
sentences = re.sub(r'(?:^| )\w(?:$| )', ' ', sentences).strip()
# lower all characters
sentences = sentences.lower()
all_sent=nltk.sent_tokenize(sentences)
all_words=[nltk.word_tokenize(sent) for sent in all_sent]
from nltk.corpus import stopwords
for i in range(len(all_words)):
  all_words[i]=[w for w in all_words[i] if w not in
stopwords.words('english')]
data =all_words
data1=data[0]


model1 = gensim.models.Word2Vec(data, min_count = 1,vector_size = 52, window= 5)


vocabulary=model1.wv.vocab
print(vocabulary)


rd='door'
#wrd=['subset','machine', 'learning','closely','related']
v1=model1.wv[wrd]
similar_words=model1.wv.most_similar(wrd)
for x in similar_words:
  print(x)
  
  
  dat = []
for i in range(2, len(data) - 2):
  context = [data1[i - 2], data1[i - 1], data1[i+1], data1[i + 2]]
  target = data1[i]
  dat.append((context, target))
print(dat[:5])



#print(dat[1][0])
[]
#for val in dat:
# print(val[0],val[1])
i=3
print(dat[i][0],dat[i][1])
print(model1.predict_output_word(dat[i][0]))


from sklearn.decomposition import PCA
from matplotlib import pyplot
X = model1[model1.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(X)
pyplot.scatter(result[:, 0], result[:, 1])
words = list(model1.wv.vocab)
for i, word in enumerate(words):
  pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()


Explanation :


 The provided code performs several tasks related to natural language processing and word embeddings using the Gensim library. Here's a step-by-step explanation of the code:

1. Import Libraries:
   - The code begins by importing various libraries, including Matplotlib, Seaborn, NumPy, Matplotlib.pyplot, NLTK, Gensim, and others.

2. Web Scraping:
   - The code scrapes text data from the Wikipedia page on "Machine learning" using urllib and Beautiful Soup (bs4).
   - The content of the web page is extracted and stored in the variable `article_text`.

3. Text Preprocessing:
   - The text data is cleaned and preprocessed in the following ways:
     - Removing special characters.
     - Removing one-letter words.
     - Converting all text to lowercase.
   - The preprocessed text is stored in the variable `sentences`.

4. Tokenization and Stopword Removal:
   - The code tokenizes the text into sentences and then into words using NLTK's tokenization functions.
   - Stopwords are removed from the words to filter out common and less meaningful words.

5. Word Embeddings with Word2Vec:
   - A Word2Vec model is trained on the preprocessed data.
   - The Word2Vec model is defined with parameters like `min_count`, `size`, and `window`.
   - The vocabulary of the Word2Vec model is printed to show the unique words.

6. Word Similarity:
   - A specific word, 'door' (`wrd`), is selected to find similar words from the trained Word2Vec model.
   - The most similar words to 'door' are printed, along with their similarity scores.

7. Context-Target Data Preparation:
   - The code prepares context-target pairs for training a neural network.
   - It iterates through the data and creates a list of context words and their corresponding target word.
   - A context consists of two words before and two words after the target word.
   - The resulting `dat` list contains tuples of context and target pairs.

8. Word Prediction with Word2Vec:
   - The code selects a specific data point (context and target pair) from `dat`.
   - It prints the context and target words.
   - It then uses the Word2Vec model to predict the output word based on the provided context.

9. Visualization with PCA:
   - The Word2Vec model is visualized using Principal Component Analysis (PCA) to reduce the dimensions of word vectors.
   - Words are plotted in a scatter plot in 2D space.
   - Words are labeled and displayed on the plot.

Overall, the code demonstrates various aspects of word embeddings using Word2Vec, including training a Word2Vec model, finding similar words, preparing data for a neural network, and visualizing word vectors in a lower-dimensional space. This code is a comprehensive example of text processing and word embedding techniques.


